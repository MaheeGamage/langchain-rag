# Session 2026-02-23 — 003

## Goal
Revamp the codebase to support multiple LLM/embedding providers (Ollama, OpenAI, Gemini)
via a single `PROVIDER` config switch, and ensure ChromaDB stores embeddings from different
models in separate named collections.

## Prompts Summary
- User asked how to check what's inside ChromaDB (SQLite-backed).
- User asked how ingest works and whether two embedding models can coexist in the same DB.
- User requested a full codebase revamp: single-switch multi-provider support, centralised
  config, factory pattern, and per-embedding-model ChromaDB collections.

## Actions Taken

### Files Modified
- **`app/config.py`** — Full rewrite. Replaced flat per-model constants with a
  `_PROVIDER_DEFAULTS` dict keyed by provider name. Added `PROVIDER` (from env),
  `COLLECTION_NAME` (derived from `EMBEDDING_MODEL`), `API_KEY`, `BASE_URL`.
  Removed old `LLM_PROVIDER`, `GEMINI_*`, `OLLAMA_BASE_URL` top-level vars.

- **`app/ingest.py`** — Removed hardcoded `OllamaEmbeddings` import. Now imports
  `get_embeddings` from `factory.py` and passes `collection_name=COLLECTION_NAME`
  to `Chroma(...)`. Also prints provider/model/collection at ingest time.

- **`app/retriever.py`** — Simplified. Removed all provider-specific branches.
  Now calls `get_embeddings()` and passes `collection_name=COLLECTION_NAME`.

- **`app/graph.py`** — Removed `OllamaLLM`, `ChatGoogleGenerativeAI`, and all
  `if LLM_PROVIDER ==` branches. Now calls `get_llm()` from `factory.py`.

- **`pyproject.toml`** — Added `langchain-openai (>=0.3.0,<0.4.0)` dependency.

- **`.env.example`** — Rewrote to show `PROVIDER=` switch and all three provider
  blocks (Ollama, OpenAI, Gemini) with their per-model override vars.

- **`AGENTS.md`** — Updated sections 1, 3, 4 (Configuration + new Provider/LLM
  factory subsection), 5 (added factory rule), 7 (updated "what not to do" for
  provider-specific classes and `if PROVIDER` branches).

### Files Created
- **`app/factory.py`** — New file. `get_llm()` and `get_embeddings()` functions.
  The only place in the codebase that imports `langchain_ollama`, `langchain_openai`,
  `langchain_google_genai`. Each function dispatches on `PROVIDER`.

### Commands Run
- `poetry add "langchain-openai>=0.3.0,<0.4.0"` — installed into `.venv`.
- Import validation via venv Python — all modules resolved cleanly.

## Outcome
Single `PROVIDER=ollama|openai|gemini` in `.env` now changes both the LLM and
embedding model. Each embedding model writes to its own ChromaDB collection
(`COLLECTION_NAME`), so multiple providers can coexist in the same `chroma_db/`
without mixing vectors. Adding a new provider requires only changes to
`config.py` and `factory.py`.

## Agent
GitHub Copilot (Claude Sonnet 4.6)
